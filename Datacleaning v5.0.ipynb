{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f22000-2be0-4cd5-888b-4475abab5f47",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-size:50px;\">Data Cleaning</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6227b1f-ce5c-4946-9294-85b65a2b23e6",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### Purpose: Loading Library\n",
    "\n",
    " - **pandas**: For handling datasets (loading, cleaning, saving).\n",
    " - **re**: For text pattern matching and removal (regex-based cleaning).\n",
    " - **ENGLISH_STOP_WORDS**: To remove common stopwords.\n",
    " - **WordNetLemmatizer**: For reducing words to their root form (lemmatization).\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117e1a10-5a7a-4a94-a03b-897c7470b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43915d9c-0d35-46bb-88cb-84dd695d6c6c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Purpose: Cleans raw text by\n",
    "\n",
    " - Removing URLs, mentions, hashtags, single letters, punctuation, and numbers.\n",
    " - Converts text to lowercase.\n",
    " - Expands contractions (e.g., \"n't\" → \"not\").\n",
    " - Removes stopwords (like \"the\", \"is\").\n",
    " - Lemmatizes words if specified (e.g., “running” → “run”).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d6db8c-1cee-481a-9d91-98d0459dc9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Preprocessing function\n",
    "# -----------------------------------------------------------------------\n",
    "def preprocess_text(text, lemmatize=False):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)      # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)         # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)         # Remove hashtags\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text) # Remove single characters (like s, t, m)\n",
    "    text = text.replace(\"n't\", \" not\").replace(\"'re\", \" are\").replace(\"'m\", \" am\").replace(\"'s\", \" is\")\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)     # Remove punctuation/numbers\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4e636-9199-43ea-ac79-476e360bb167",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Purpose:\n",
    "\n",
    " - Defines file paths for raw datasets and cleaned datasets.\n",
    " - Ensures consistent references across cleaning functions.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c9624a-bfd9-452c-8f3c-9aa21bda0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# File paths (raw and cleaned)\n",
    "# -------------------------------------------------------------------------\n",
    "sentiment140_path = \"Sentiment140.csv\"\n",
    "balanced_path = \"balanced_sentiment_dataset.csv\"\n",
    "million_path = \"milliondataset.csv\"\n",
    "\n",
    "clean_sentiment140_path = \"Sentiment140_clean.csv\"\n",
    "clean_balanced_path = \"balanced_sentiment_dataset_clean.csv\"\n",
    "clean_million_path = \"milliondataset_clean.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf12c00-39e7-49dd-9d7f-cc718a78b15b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Purpose:\n",
    "\n",
    " - Loads raw Sentiment140 dataset.[Only Column needed for analysis to save the processing space\n",
    " - Maps target values: 0 → negative, 1 → positive.\n",
    " - Cleans text using preprocess_text.\n",
    " - Saves the cleaned dataset with only essential columns.\n",
    "   - New Dataset Name: **\"*Sentiment140_clean.csv*\"**\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f978d13-e328-4a17-82c2-33c1c133c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Clean Sentiment140 in chunks\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def clean_sentiment140():\n",
    "    # Load entire dataset\n",
    "    df = pd.read_csv(sentiment140_path, encoding='latin-1', header=None)\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={0: \"target\", 5: \"text\"})\n",
    "    # Map 4 → 1 (positive)\n",
    "    df[\"target\"] = df[\"target\"].replace(4, 1)\n",
    "    # Clean text\n",
    "    df[\"Cleaned text\"] = df[\"text\"].apply(preprocess_text)\n",
    "    # Keep only required columns\n",
    "    df = df[[\"Cleaned text\", \"target\"]]\n",
    "    # Save cleaned dataset\n",
    "    df.to_csv(clean_sentiment140_path, index=False)\n",
    "    print(f\"Cleaned Sentiment140 saved to {clean_sentiment140_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50318222-59c9-4b9a-8995-250e168815d8",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### Purpose:\n",
    "\n",
    " - Cleans Balanced sentiment dataset.\n",
    " - Renames columns and preprocesses text.\n",
    " - Saves cleaned file with only required columns.\n",
    "    - New Dataset Name : **\"*balanced_sentiment_dataset_clean.csv*\"**\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ba70a0-57c0-4fcc-9df9-4ce2035b7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2. Clean Balanced dataset\n",
    "# ------------------------------------------------------------------------\n",
    "def clean_balanced():\n",
    "    df = pd.read_csv(balanced_path)\n",
    "    df = df.rename(columns={\"sentiment\": \"target\", \"text\": \"text\"})\n",
    "    df[\"Cleaned text\"] = df[\"text\"].apply(preprocess_text)\n",
    "    df[[\"Cleaned text\", \"target\"]].to_csv(clean_balanced_path, index=False)\n",
    "    print(f\"Cleaned Balanced dataset saved to {clean_balanced_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fadb0-86e1-470d-9a89-265f75d8c1b5",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### Purpose:\n",
    "\n",
    " - Cleans Million dataset.\n",
    " - Filters only English and sentiment-labeled data.\n",
    " - Maps sentiment strings to numeric targets (negative=0, positive=1).\n",
    " - Preprocesses text and saves cleaned dataset.\n",
    "    - New Dataset Name : **\"*milliondataset_clean.csv*\"**\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8164b0d-5352-40ae-b668-8dc82b04fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# 3. Clean Million dataset\n",
    "# -----------------------------------------------------------------------\n",
    "def clean_million():\n",
    "    df = pd.read_csv(million_path)\n",
    "    df = df[df[\"Language\"] == \"en\"]\n",
    "    df = df[df[\"Label\"].isin([\"negative\", \"positive\"])]\n",
    "    df[\"target\"] = df[\"Label\"].map({\"negative\": 0, \"positive\": 1})\n",
    "    df[\"Cleaned text\"] = df[\"Text\"].apply(preprocess_text)\n",
    "    df[[\"Cleaned text\", \"target\"]].to_csv(clean_million_path, index=False)\n",
    "    print(f\"Cleaned Million dataset saved to {clean_million_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251418f-7234-4c7b-823f-81f2a337f3cf",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "## Execute All Cleaning Functions\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b2e0b4-70df-42cb-bd5f-ea9cccdb2f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Sentiment140 saved to Sentiment140_clean.csv\n",
      "Cleaned Balanced dataset saved to balanced_sentiment_dataset_clean.csv\n",
      "Cleaned Million dataset saved to milliondataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Run cleaning\n",
    "# -------------------\n",
    "clean_sentiment140()\n",
    "clean_balanced()\n",
    "clean_million()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8f74f-e3c0-4c74-a813-023776b8f4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
